{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "np.random.seed(42) # 乱数生成用のシードを設定\n",
    "ite = 1 # 乱数生成の回数\n",
    "\n",
    "random_seed = np.random.randint(0, 10000, ite)  # ランダムな整数値をシード値として取得.例えば 0 〜 9999 の間の整数をite個生成\n",
    "print(\"random_seed\", random_seed) # 乱数シード値の確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "\n",
    "# 時間埋め込み（正弦波位置エンコーディング）\n",
    "def pos_encoding(timesteps, output_dim, device='cpu'):\n",
    "    position = timesteps.view(-1, 1).float()  # 必要に応じて型変換\n",
    "    div_term = torch.exp(torch.arange(0, output_dim, 2, device=device, dtype=torch.float32) * \n",
    "                         (-np.log(10000.0) / output_dim))\n",
    "    sinusoid = torch.cat([torch.sin(position * div_term), torch.cos(position * div_term)], dim=1)\n",
    "    return sinusoid\n",
    "\n",
    "# Dropoutの導入: 過学習を防ぐために、各隠れ層にnn.Dropoutを追加。\n",
    "# Batch Normalizationの導入: 学習を安定させるためにnn.BatchNorm1dを適用。\n",
    "# 活性化関数の選択: F.reluの代わりにnn.LeakyReLUやnn.ELUを試すことで、勾配消失問題に対応。\n",
    "\n",
    "# 拡散モデル\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, time_embed_dim=16):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.time_embed_dim = time_embed_dim  # time_embed_dimをインスタンス変数として初期化\n",
    "        self.fc1 = nn.Linear(2 + time_embed_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # 時間埋め込み\n",
    "        t_embed = pos_encoding(t, self.time_embed_dim, x.device)\n",
    "        x_t = torch.cat([x, t_embed], dim=-1)  # 時間情報と入力データを結合\n",
    "        x_t = F.relu(self.fc1(x_t))\n",
    "        x_t = F.relu(self.fc2(x_t))\n",
    "        return self.fc3(x_t)\n",
    "\n",
    "# 拡散プロセス\n",
    "class Diffuser:\n",
    "    def __init__(self, num_timesteps=1000, beta_start=0.0001, beta_end=0.02, device='cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def add_noise(self, x_0, t):\n",
    "        t_idx = t - 1 # alphas[0] is for t=1\n",
    "        alpha_bar = self.alpha_bars[t_idx].view(-1, 1)  # (N, 1)\n",
    "        noise = torch.randn_like(x_0, device=self.device)\n",
    "        x_t = torch.sqrt(alpha_bar) * x_0 + torch.sqrt(1 - alpha_bar) * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    def denoise(self, model, x, t):\n",
    "        T = self.num_timesteps\n",
    "        assert (t >= 1).all() and (t <= T).all()\n",
    "        \n",
    "        t_idx = t - 1 # alphas[0] is for t=1\n",
    "        alpha = self.alphas[t_idx].view(-1, 1)\n",
    "        alpha_bar = self.alpha_bars[t_idx].view(-1, 1)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            eps = model(x, t)\n",
    "\n",
    "        noise = torch.randn_like(x, device=self.device)\n",
    "        noise[t == 1] = 0  # no noise at t=1\n",
    "\n",
    "        mu = (x - (1 - alpha) / torch.sqrt(1 - alpha_bar) * eps) / torch.sqrt(alpha)\n",
    "\n",
    "        return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例4: 比率30:70、円形クラスタ\n",
    "print(\"--- Example 4: 30:70 Ratio, Circular Clusters ---\")\n",
    "dataset4 = generate_uncorrelated_gmm_2d_toy_dataset(\n",
    "    num_total_samples=2000,\n",
    "    ratio_mode1=0.3,\n",
    "    mu1=np.array([-3.0, -3.0]),\n",
    "    sigma1_diag=np.array([1.0, 1.0]),\n",
    "    mu2=np.array([3.0, 3.0]),\n",
    "    sigma2_diag=np.array([1.0, 1.0])\n",
    ")\n",
    "plot_2d_dataset(dataset4, title=\"Uncorrelated GMM (30:70, Circular)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "\n",
    "# 2次元正規分布の平均ベクトルと共分散行列を設定\n",
    "original_mean = [5, 5]  # 平均ベクトル\n",
    "original_cov = [[1, 0.3], [0.3, 1]]  # 共分散行列（相関あり）\n",
    "\n",
    "# データセットサイズとデータセット数\n",
    "num_dataset = 1000 # サンプリング回数\n",
    "dataset_size = 500 # 元データのサイズ \n",
    "\n",
    "\n",
    "# ハイパーパラメータ\n",
    "num_timesteps = 500 # 拡散ステップ数\n",
    "epochs = 10          # 学習エポック数\n",
    "lr = 1e-3         # 学習率\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# シード値の固定\n",
    "np.random.seed(42)\n",
    "\n",
    "# サンプリング\n",
    "iter = 1 #学習元データの数\n",
    "# シード値の生成\n",
    "random_seed = np.random.randint(0, 10000, iter)\n",
    "\n",
    "# モデルとデータを管理する辞書\n",
    "models = {}\n",
    "original_datas = {}\n",
    "\n",
    "for (i, seed) in enumerate(random_seed):\n",
    "    start_time = time.time() # 計測開始\n",
    "\n",
    "    # モデルの初期化\n",
    "    time_embed_dim = 16\n",
    "    model = DiffusionModel(time_embed_dim=time_embed_dim).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    diffuser = Diffuser(num_timesteps=num_timesteps, device=device)\n",
    "\n",
    "    # 学習データ(ガウスノイズ)\n",
    "    print(\"############################################\")\n",
    "    print(f\"Data_Set_{i+1}, Seed: {seed}\") # 開始の合図\n",
    "\n",
    "    print(f\"拡散ステップ数: {num_timesteps}, 学習エポック数: {epochs}, 学習率: {lr}\")\n",
    "    print(\"############################################\")\n",
    "    np.random.seed(seed) # 取得した乱数を新しいシード値として設定\n",
    "    data = np.random.multivariate_normal(original_mean, original_cov, size=dataset_size) # 学習元データの生成 (50, 2)\n",
    "    # データの標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data) # shape: (500, 2), dtype: float64\n",
    "\n",
    "    print(\"data.shape\", data.shape) # (50, 2)\n",
    "    # PyTorchテンソルへ明示的に float32 で変換\n",
    "    train_data = torch.tensor(scaled_data, dtype=torch.float32).to(device) # shape: (50, 2)\n",
    "\n",
    "    # データローダー作成\n",
    "    batch_size = 10\n",
    "    # モデル学習に使う DataLoader も float32 のテンソルから作成\n",
    "    dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # 原点を基準として赤十字\n",
    "    plt.axhline(original_mean[0], color='gray', linestyle='--')\n",
    "    plt.axvline(original_mean[1], color='gray', linestyle='--')\n",
    "    plt.scatter(original_mean[0], original_mean[1], color='red', marker='x', s=100, label='True Mean')\n",
    "    # データの可視化\n",
    "    plt.scatter(data[:, 0], data[:, 1], label='Original Data')\n",
    "    plt.title(f'Original Data (Seed: {seed})')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.legend()\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # 標準化したデータの可視化\n",
    "    plt.scatter(scaled_data[:, 0], scaled_data[:, 1], label='Scaled Data')\n",
    "    plt.title(f'Scaled Data (Seed: {seed})')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.legend()\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # データの要約\n",
    "    print(\"dataの平均ベクトル\", np.mean(data, axis=0)) # 平均ベクトル\n",
    "    print(\"dataの分散共分散行列\", np.cov(data.T)) # 分散共分散行列\n",
    "    print(\"dataの相関係数\", np.corrcoef(data.T)) # 相関係数\n",
    "\n",
    "    # 学習\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0.0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x = batch.to(device)\n",
    "            t = torch.randint(1, num_timesteps + 1, (len(x),), device=device)\n",
    "\n",
    "            x_noisy, noise = diffuser.add_noise(x, t)\n",
    "            noise_pred = model(x_noisy, t)\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "        avg_loss = loss_sum / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        # 5の倍数エポックで損失を表示\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss}\")\n",
    "    # 辞書に保存\n",
    "    models[f\"model_{i+1}\"] = model\n",
    "    original_datas[f\"seed_{seed}\"] = data\n",
    "    print(\"学習終了\")\n",
    "    end_time = time.time() # 計測終了\n",
    "    print('\\n')\n",
    "    print(f\"学習時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "    # # モデルの保存\n",
    "    # torch.save(model.state_dict(), f\"model_{i+1}.pth\")\n",
    "\n",
    "    # 学習曲線のプロット\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Trained By data_by_seed_{}'.format(seed))\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    print(\"#\"*50)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "\n",
    "\n",
    "# 拡散モデルのサンプリング関数\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_samples(model, n=dataset_size, B=num_dataset, device='cpu'):\n",
    "    model.eval() # モデルを評価モードに設定\n",
    "    with torch.no_grad(): # 勾配計算を無効化\n",
    "        new_sample_list = [] # サンプルを格納するリスト\n",
    "        for _ in tqdm(range(B)): # 進捗バーを表示\n",
    "            torch.manual_seed(np.random.randint(0, 10000)) # ランダムシードを設定\n",
    "            samples = torch.randn((n, 2), device=device) # サンプルを生成\n",
    "            for t in range(num_timesteps, 0, -1): # 拡散ステップを逆に進める\n",
    "                t_tensor = torch.full((n,), t, device=device, dtype=torch.long) # 時間情報をテンソルに変換\n",
    "                samples = diffuser.denoise(model, samples, t_tensor) # ノイズ除去\n",
    "            new_sample_list.append(samples.cpu().numpy()) # CPUに戻してnumpy配列に変換\n",
    "        return np.array(new_sample_list)  # (B, n, 2) \n",
    "\n",
    "\n",
    "# ブートストラップサンプリングの実行\n",
    "generated_data_list = []\n",
    "\n",
    "\n",
    "# 拡散モデルによるサンプリング\n",
    "for model_key, data, seed, selected_model in zip(original_datas.keys(), original_datas.values(), models.keys(), models.values()):\n",
    "    print(\"-\"*50)\n",
    "    print(\"#\"*50)\n",
    "    print(f\"Seed: {seed.split('_')[-1]}\")\n",
    "    print(\"サンプリング開始\")\n",
    "\n",
    "    start_time = time.time() # 計測開始\n",
    "    generated_data = generate_samples(selected_model, n=dataset_size, B=num_dataset, device=device) # サンプリング実行\n",
    "    # scalerを使ったならば、逆変換を行う\n",
    "    if scaler:\n",
    "        generated_data = scaler.inverse_transform(generated_data.reshape(-1, 2)).reshape(num_dataset, dataset_size, 2)\n",
    "    generated_data_list.append(generated_data) # サンプルをリストに追加\n",
    "    generated_data = np.array(generated_data) # numpy配列に変換\n",
    "    print(\"generated_data.shape\", generated_data.shape) # (B, n, 2)\n",
    "    end_time = time.time() # 計測終了\n",
    "    print(\"サンプリング終了\")\n",
    "\n",
    "    print(f\"サンプリング時間: {end_time - start_time:.2f}秒\")\n",
    "    print(f\"サンプリング時間: {(end_time - start_time)//60}分 {(end_time - start_time)%60}秒\")\n",
    "\n",
    "    # サンプルされたデータの保存\n",
    "    # torch.save(generated_data, f\"master_research/saved_data/sampled_data/sampled_data_{seed.split('_')[-1]}_epoch_{epochs}.pth\")\n",
    "    print(\"#\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generated_data_list[0].shape\", generated_data_list[0].shape) # (1000, 50, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_data: shape (B, n, 2)\n",
    "# scaler: 学習時に fit() 済の StandardScaler オブジェクト\n",
    "\n",
    "generated_data = generated_data_list[0]\n",
    "\n",
    "# 逆変換を適用\n",
    "generated_data_original = np.array([\n",
    "    scaler.inverse_transform(sample) for sample in generated_data\n",
    "])  # shape: (B, n, 2)\n",
    "\n",
    "generated_data_list[0] = generated_data_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットごとに図示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generated_data_list[0].shape\", generated_data_list[0].shape)  # (1000, 50, 2)\n",
    "# サンプリングされたデータの可視化\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(generated_data_list[0][:, 1, 0].flatten(), generated_data_list[0][:, 1, 1].flatten(), alpha=0.5, label='Generated Samples')\n",
    "plt.title('Generated Samples from Diffusion Model')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.axhline(original_mean[0], color='gray', linestyle='--')\n",
    "plt.axvline(original_mean[1], color='gray', linestyle='--')\n",
    "plt.scatter(original_mean[0], original_mean[1], color='red', marker='x', s=100, label='True Mean')\n",
    "plt.xlim(9.0, 11)\n",
    "plt.ylim(9.0, 11)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sampled_data = generate_samples(models['model_1'], n=dataset_size, B=50, device=device)\n",
    "# 逆変換を適用\n",
    "sample_sampled_data = scaler.inverse_transform(sample_sampled_data.reshape(-1, 2)).reshape(1, dataset_size, 2)  # shape: (1, 500, 2)\n",
    "x_mean_sample_sampled_data = np.mean(sample_sampled_data[0], axis=0)  # サンプルされたデータのx平均\n",
    "y_mean_sample_sampled_data = np.mean(sample_sampled_data[0], axis=1)  # サンプルされたデータのy平均\n",
    "# サンプルされたデータの可視化\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(sample_sampled_data[0][:, 0], sample_sampled_data[0][:, 1], alpha=0.5, label='Sampled Data')\n",
    "plt.title('Sampled Data from Diffusion Model')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.axhline(original_mean[0], color='gray', linestyle='--')\n",
    "plt.axvline(original_mean[1], color='gray', linestyle='--')\n",
    "plt.scatter(original_mean[0], original_mean[1], color='red', marker='x', s=100, label='True Mean')\n",
    "# plt.xlim(x_mean_sample_sampled_data-1, x_mean_sample_sampled_data+1)\n",
    "# plt.ylim(y_mean_sample_sampled_data-1, y_mean_sample_sampled_data+1)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットごとに平均値をとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_data_original: shape (1000, 50, 2)\n",
    "# 生成されたデータのデータセットごとの平均ベクトルと分散共分散行列を計算\n",
    "means_of_generated_data = np.mean(generated_data_original, axis=1)  # shape: (1000, 2)\n",
    "# print(\"生成されたデータのデータセットごとの平均ベクトル(means_of_generated_data)\", means_of_generated_data)\n",
    "print(\"生成されたデータのデータセットごとの平均ベクトル(means_of_generated_data)のshape\", means_of_generated_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可視化\n",
    "x_vals = means_of_generated_data[:, 0].flatten()\n",
    "y_vals = means_of_generated_data[:, 1].flatten()\n",
    "\n",
    "# 最小・最大 + 標準偏差から範囲を決定\n",
    "x_min, x_max = x_vals.min(), x_vals.max()\n",
    "y_min, y_max = y_vals.min(), y_vals.max()\n",
    "x_std, y_std = x_vals.std(), y_vals.std()\n",
    "\n",
    "k = 0.5  # 余白のスケール（標準偏差単位）\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_vals, y_vals, alpha=0.5, label='Generated Data')\n",
    "plt.title('Generated Data (Seed: {})'.format(random_seed[0]))\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "\n",
    "# 拡張された描画範囲\n",
    "plt.xlim(x_min - k * x_std, x_max + k * x_std)\n",
    "plt.ylim(y_min - k * y_std, y_max + k * y_std)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_data_original: shape (1000, 50, 2)\n",
    "# 生成されたデータのデータセットごとの相関係数を計算\n",
    "covs_of_generated_data = np.array([np.cov(sample.T) for sample in generated_data_original])  # shape: (1000, 2, 2)\n",
    "# 相関係数を計算\n",
    "correlation_coeffs = np.array([np.corrcoef(sample.T) for sample in generated_data_original])  # shape: (1000, 2, 2)\n",
    "# print(\"生成されたデータのデータセットごとの相関係数(covs_of_generated_data)\", covs_of_generated_data)\n",
    "print(\"生成されたデータのデータセットごとの相関係数(covs_of_generated_data)のshape\", covs_of_generated_data.shape)\n",
    "\n",
    "# # 相関係数の分布\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.hist(covs_of_generated_data, bins=30, color='orchid')\n",
    "# plt.title('Diffusion Model Sampling Correlation Coefficients')\n",
    "# plt.xlabel('Correlation Coefficient')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 必要\n",
    "\n",
    "\n",
    "# サンプリングの可視化\n",
    "for seed, generated_samples in zip(random_seed, generated_data_list):\n",
    "    print(\"############################################\")\n",
    "    print(\"Random Seed:\", seed) # 開始の合図\n",
    "    print(\"############################################\")\n",
    "\n",
    "    # ---------------------統計量制御---------------------\n",
    "    generated_samples_mean_vecs = np.mean(generated_samples, axis=1) # 拡散モデルによるサンプルの平均ベクトル（1000, 2）\n",
    "    generated_samples_cov_mats = np.array([np.cov(sample.T) for sample in generated_samples]) # 拡散モデルによるサンプルの共分散行列（1000, 2, 2）\n",
    "    generated_samples_corr_coefs = np.array([np.corrcoef(sample.T)[0, 1] for sample in generated_samples]) # 拡散モデルによるサンプルの相関係数（1000,）\n",
    "\n",
    "    # サイズの確認\n",
    "    # print(\"Bootstrap Mean Vector\", bootstrap_mean_vecs.shape) # 平均ベクトル\n",
    "    # print(\"Bootstrap Covariance Matrix\", bootstrap_cov_mats.shape) # 共分散行列\n",
    "    # print(\"Bootstrap Correlation Coefficient\", bootstrap_corr_coefs.shape) # 相関係数\n",
    "\n",
    "    # 代表値を出力\n",
    "    print(\"平均ベクトルの平均:\", np.mean(generated_samples_mean_vecs, axis=0))\n",
    "    print(\"共分散行列の平均:\\n\", np.mean(generated_samples_cov_mats, axis=0))\n",
    "    print(\"相関係数の平均:\", np.mean(generated_samples_corr_coefs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 平均ベクトルの分布\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(generated_samples_mean_vecs[:, 0], generated_samples_mean_vecs[:, 1], alpha=0.5, color='teal', label='Diffusion Model Sampling Means')\n",
    "\n",
    "    # 原点を基準として赤十字\n",
    "    plt.axhline(original_mean[0], color='gray', linestyle='--')\n",
    "    plt.axvline(original_mean[1], color='gray', linestyle='--')\n",
    "    plt.scatter(original_mean[0], original_mean[1], color='red', marker='x', s=100, label='True Mean')\n",
    "\n",
    "    plt.xlabel('Mean of X1')\n",
    "    plt.ylabel('Mean of X2')\n",
    "    plt.title('Scatter Plot of Diffusion Model Sampling Mean Vectors')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ## 可視化\n",
    "    x_vals = means_of_generated_data[:, 0].flatten()\n",
    "    y_vals = means_of_generated_data[:, 1].flatten()\n",
    "\n",
    "    # 最小・最大 + 標準偏差から範囲を決定\n",
    "    x_min, x_max = x_vals.min(), x_vals.max()\n",
    "    y_min, y_max = y_vals.min(), y_vals.max()\n",
    "    x_std, y_std = x_vals.std(), y_vals.std()\n",
    "\n",
    "    k = 0.5  # 余白のスケール（標準偏差単位）\n",
    "\n",
    "    # プロット\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_vals, y_vals, alpha=0.5, label='Generated Data')\n",
    "    plt.title('Generated Data (Seed: {})'.format(seed))\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "\n",
    "    # 拡張された描画範囲\n",
    "    plt.xlim(x_min - k * x_std, x_max + k * x_std)\n",
    "    plt.ylim(y_min - k * y_std, y_max + k * y_std)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # カーネル密度推定\n",
    "    kde = gaussian_kde(generated_samples_mean_vecs.T)\n",
    "\n",
    "    # グリッド生成\n",
    "    x = np.linspace(np.min(generated_samples_mean_vecs[:, 0]) - 0.5, np.max(generated_samples_mean_vecs[:, 0]) + 0.5, 100)\n",
    "    y = np.linspace(np.min(generated_samples_mean_vecs[:, 1]) - 0.5, np.max(generated_samples_mean_vecs[:, 1]) + 0.5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "    Z = kde(positions).reshape(X.shape)\n",
    "\n",
    "    # 3Dプロット\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.9)\n",
    "\n",
    "    ax.set_xlabel('Mean of X1')\n",
    "    ax.set_ylabel('Mean of X2')\n",
    "    ax.set_zlabel('Density')\n",
    "    ax.set_title('Bird\\'s Eye View of Diffusion Model Sampling Mean Vectors (KDE)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 相関係数の分布\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(generated_samples_corr_coefs, bins=30, color='orchid')\n",
    "    plt.title('Diffusion Model Sampling Correlation Coefficients')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 終了の合図\n",
    "    print(\"############################################\")\n",
    "    print(\"End\")\n",
    "    print(\"############################################\")\n",
    "\n",
    "    # 改行\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMMを試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## いろんなGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_gmm_2d_toy_dataset(num_samples_per_mode=500,\n",
    "                                mu1=np.array([-3.0, -3.0]),\n",
    "                                sigma1=np.array([[1.0, 0.5], [0.5, 1.0]]),\n",
    "                                mu2=np.array([3.0, 3.0]),\n",
    "                                sigma2=np.array([[1.0, -0.5], [-0.5, 1.0]]),\n",
    "                                random_seed=42):\n",
    "    \"\"\"\n",
    "    2つの2次元正規分布を組み合わせたガウス混合モデル (GMM) からトイデータセットを生成します。\n",
    "\n",
    "    Args:\n",
    "        num_samples_per_mode (int): 各正規分布から生成するサンプル数。\n",
    "        mu1 (np.ndarray): 1つ目の正規分布の平均ベクトル (2次元)。\n",
    "        sigma1 (np.ndarray): 1つ目の正規分布の共分散行列 (2x2)。\n",
    "        mu2 (np.ndarray): 2つ目の正規分布の平均ベクトル (2次元)。\n",
    "        sigma2 (np.ndarray): 2つ目の正規分布の共分散行列 (2x2)。\n",
    "        random_seed (int): 乱数生成のシード。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 生成されたデータポイント (N x 2)。\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # 1つ目の正規分布からデータを生成\n",
    "    data_mode1 = np.random.multivariate_normal(mu1, sigma1, num_samples_per_mode)\n",
    "\n",
    "    # 2つ目の正規分布からデータを生成\n",
    "    data_mode2 = np.random.multivariate_normal(mu2, sigma2, num_samples_per_mode)\n",
    "\n",
    "    # 両方のデータを結合\n",
    "    dataset = np.vstack((data_mode1, data_mode2))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def plot_2d_dataset(dataset, title=\"2D GMM Toy Dataset\"):\n",
    "    \"\"\"\n",
    "    2次元データセットをプロットします。\n",
    "\n",
    "    Args:\n",
    "        dataset (np.ndarray): プロットするデータセット (N x 2)。\n",
    "        title (str): グラフのタイトル。\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(dataset[:, 0], dataset[:, 1], s=10, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal') # x軸とy軸のスケールを合わせる\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # データセットの生成\n",
    "    # 例1: 離れた2つのクラスタ\n",
    "    print(\"Generating Dataset 1: Two distinct clusters...\")\n",
    "    dataset1 = generate_gmm_2d_toy_dataset(\n",
    "        num_samples_per_mode=1000,\n",
    "        mu1=np.array([-5.0, -5.0]),\n",
    "        sigma1=np.array([[1.5, 0.2], [0.2, 1.5]]),\n",
    "        mu2=np.array([5.0, 5.0]),\n",
    "        sigma2=np.array([[1.5, -0.2], [-0.2, 1.5]])\n",
    "    )\n",
    "    plot_2d_dataset(dataset1, title=\"2D GMM Toy Dataset (Distinct Clusters)\")\n",
    "\n",
    "    # 例2: 少し重なり合う2つのクラスタ\n",
    "    print(\"\\nGenerating Dataset 2: Two slightly overlapping clusters...\")\n",
    "    dataset2 = generate_gmm_2d_toy_dataset(\n",
    "        num_samples_per_mode=750,\n",
    "        mu1=np.array([-2.0, 0.0]),\n",
    "        sigma1=np.array([[1.0, 0.8], [0.8, 1.0]]),\n",
    "        mu2=np.array([2.0, 0.0]),\n",
    "        sigma2=np.array([[1.0, -0.8], [-0.8, 1.0]])\n",
    "    )\n",
    "    plot_2d_dataset(dataset2, title=\"2D GMM Toy Dataset (Slightly Overlapping Clusters)\")\n",
    "\n",
    "    # 例3: 異なるサイズのクラスタ (サンプル数を変える)\n",
    "    print(\"\\nGenerating Dataset 3: Clusters with different number of samples...\")\n",
    "    # num_samples_per_modeを引数で渡すが、実際には異なる数で生成する場合は関数を少し変更するか、\n",
    "    # 各modeのデータ生成を別々に呼び出す必要がある。\n",
    "    # ここでは便宜上、同じ数で生成。\n",
    "    dataset3_mode1 = np.random.multivariate_normal(np.array([0.0, 4.0]), np.array([[0.8, 0.1], [0.1, 0.8]]), 300)\n",
    "    dataset3_mode2 = np.random.multivariate_normal(np.array([0.0, -4.0]), np.array([[2.0, 0.0], [0.0, 0.5]]), 1500)\n",
    "    dataset3 = np.vstack((dataset3_mode1, dataset3_mode2))\n",
    "    plot_2d_dataset(dataset3, title=\"2D GMM Toy Dataset (Different Sample Counts)\")\n",
    "\n",
    "    # 生成されたデータセットの形状を確認\n",
    "    print(f\"\\nShape of Dataset 1: {dataset1.shape}\")\n",
    "    print(f\"Shape of Dataset 2: {dataset2.shape}\")\n",
    "    print(f\"Shape of Dataset 3: {dataset3.shape}\")\n",
    "\n",
    "    # このデータセットを拡散モデルの学習ループに渡すことを想定\n",
    "    # 例: dataloader = create_dataloader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 無相関な正規分布からできたGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_uncorrelated_gmm_2d_toy_dataset(num_total_samples=2000,\n",
    "                                            ratio_mode1=0.5, # モード1のデータが全体の何割を占めるか (0.0 ~ 1.0)\n",
    "                                            mu1=np.array([-3.0, -3.0]),\n",
    "                                            sigma1_diag=np.array([1.0, 1.0]), # 対角成分のみ指定\n",
    "                                            mu2=np.array([3.0, 3.0]),\n",
    "                                            sigma2_diag=np.array([1.0, 1.0]), # 対角成分のみ指定\n",
    "                                            random_seed=42):\n",
    "    \"\"\"\n",
    "    2つの無相関な2次元正規分布を組み合わせたガウス混合モデル (GMM) からトイデータセットを生成します。\n",
    "    各正規分布の比率を指定できます。\n",
    "\n",
    "    Args:\n",
    "        num_total_samples (int): 生成する総サンプル数。\n",
    "        ratio_mode1 (float): モード1 (1つ目の正規分布) のデータが全体の何割を占めるか (0.0 ~ 1.0)。\n",
    "                              モード2の比率は (1 - ratio_mode1) になります。\n",
    "        mu1 (np.ndarray): 1つ目の正規分布の平均ベクトル (2次元)。\n",
    "        sigma1_diag (np.ndarray): 1つ目の正規分布の共分散行列の対角成分 (2要素)。\n",
    "                                  非対角成分は0と仮定されます。\n",
    "        mu2 (np.ndarray): 2つ目の正規分布の平均ベクトル (2次元)。\n",
    "        sigma2_diag (np.ndarray): 2つ目の正規分布の共分散行列の対角成分 (2要素)。\n",
    "                                  非対角成分は0と仮定されます。\n",
    "        random_seed (int): 乱数生成のシード。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 生成されたデータポイント (N x 2)。\n",
    "    \"\"\"\n",
    "    if not (0.0 <= ratio_mode1 <= 1.0):\n",
    "        raise ValueError(\"ratio_mode1 must be between 0.0 and 1.0\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # 各モードのサンプル数を計算\n",
    "    num_samples_mode1 = int(num_total_samples * ratio_mode1)\n",
    "    num_samples_mode2 = num_total_samples - num_samples_mode1 # 残りはモード2\n",
    "\n",
    "    # 共分散行列を作成 (対角成分のみ有効)\n",
    "    sigma1 = np.diag(sigma1_diag)\n",
    "    sigma2 = np.diag(sigma2_diag)\n",
    "\n",
    "    print(f\"Generating {num_total_samples} samples:\")\n",
    "    print(f\"  Mode 1 ({ratio_mode1*100:.1f}%): {num_samples_mode1} samples\")\n",
    "    print(f\"  Mode 2 ({(1-ratio_mode1)*100:.1f}%): {num_samples_mode2} samples\")\n",
    "\n",
    "    # 1つ目の正規分布からデータを生成\n",
    "    data_mode1 = np.random.multivariate_normal(mu1, sigma1, num_samples_mode1)\n",
    "\n",
    "    # 2つ目の正規分布からデータを生成\n",
    "    data_mode2 = np.random.multivariate_normal(mu2, sigma2, num_samples_mode2)\n",
    "\n",
    "    # 両方のデータを結合\n",
    "    dataset = np.vstack((data_mode1, data_mode2))\n",
    "\n",
    "    # データをシャッフル (モードの偏りをなくすため)\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def plot_2d_dataset(dataset, title=\"2D GMM Toy Dataset (Uncorrelated)\"):\n",
    "    \"\"\"\n",
    "    2次元データセットをプロットします。\n",
    "\n",
    "    Args:\n",
    "        dataset (np.ndarray): プロットするデータセット (N x 2)。\n",
    "        title (str): グラフのタイトル。\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(dataset[:, 0], dataset[:, 1], s=10, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal') # x軸とy軸のスケールを合わせる\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 例1: 比率50:50、円形クラスタ\n",
    "    print(\"--- Example 1: 50:50 Ratio, Circular Clusters ---\")\n",
    "    dataset1 = generate_uncorrelated_gmm_2d_toy_dataset(\n",
    "        num_total_samples=2000,\n",
    "        ratio_mode1=0.5,\n",
    "        mu1=np.array([-4.0, -4.0]),\n",
    "        sigma1_diag=np.array([1.0, 1.0]),\n",
    "        mu2=np.array([4.0, 4.0]),\n",
    "        sigma2_diag=np.array([1.0, 1.0])\n",
    "    )\n",
    "    plot_2d_dataset(dataset1, title=\"Uncorrelated GMM (50:50, Circular)\")\n",
    "\n",
    "    # 例2: 比率80:20、楕円形クラスタ (軸に平行)\n",
    "    print(\"\\n--- Example 2: 80:20 Ratio, Elliptical Clusters ---\")\n",
    "    dataset2 = generate_uncorrelated_gmm_2d_toy_dataset(\n",
    "        num_total_samples=3000,\n",
    "        ratio_mode1=0.8,\n",
    "        mu1=np.array([-2.0, 0.0]),\n",
    "        sigma1_diag=np.array([0.5, 2.0]), # X軸方向に狭く、Y軸方向に広い楕円\n",
    "        mu2=np.array([2.0, 0.0]),\n",
    "        sigma2_diag=np.array([2.0, 0.5])  # X軸方向に広く、Y軸方向に狭い楕円\n",
    "    )\n",
    "    plot_2d_dataset(dataset2, title=\"Uncorrelated GMM (80:20, Elliptical)\")\n",
    "\n",
    "    # 例3: 比率20:80、異なる広がり\n",
    "    print(\"\\n--- Example 3: 20:80 Ratio, Different Spreads ---\")\n",
    "    dataset3 = generate_uncorrelated_gmm_2d_toy_dataset(\n",
    "        num_total_samples=2500,\n",
    "        ratio_mode1=0.2,\n",
    "        mu1=np.array([0.0, 5.0]),\n",
    "        sigma1_diag=np.array([0.8, 0.8]), # 小さめの円\n",
    "        mu2=np.array([0.0, -5.0]),\n",
    "        sigma2_diag=np.array([3.0, 3.0])  # 大きめの円\n",
    "    )\n",
    "    plot_2d_dataset(dataset3, title=\"Uncorrelated GMM (20:80, Different Spreads)\")\n",
    "\n",
    "    # 生成されたデータセットの形状を確認\n",
    "    print(f\"\\nShape of Dataset 1: {dataset1.shape}\")\n",
    "    print(f\"Shape of Dataset 2: {dataset2.shape}\")\n",
    "    print(f\"Shape of Dataset 3: {dataset3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 試したいGMM: 比率30:70、円形クラスタ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例4: 比率30:70、円形クラスタ\n",
    "print(\"--- Example 4: 30:70 Ratio, Circular Clusters ---\")\n",
    "dataset4 = generate_uncorrelated_gmm_2d_toy_dataset(\n",
    "    num_total_samples=2000,\n",
    "    ratio_mode1=0.3,\n",
    "    mu1=np.array([-3.0, -3.0]),\n",
    "    sigma1_diag=np.array([1.0, 1.0]),\n",
    "    mu2=np.array([3.0, 3.0]),\n",
    "    sigma2_diag=np.array([1.0, 1.0])\n",
    ")\n",
    "plot_2d_dataset(dataset4, title=\"Uncorrelated GMM (30:70, Circular)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義と拡散プロセスの実装\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Diffusion Modelの定義\n",
    "time_embed_dim = 16 # 時間埋め込みの次元数\n",
    "model_4 = DiffusionModel(time_embed_dim=time_embed_dim).to(device)\n",
    "optimizer_4 = Adam(model_4.parameters(), lr=lr)\n",
    "# Diffuserのインスタンスを作成\n",
    "diffuser_4 = Diffuser(num_timesteps=num_timesteps, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの初期化\n",
    "time_embed_dim = 16\n",
    "model = DiffusionModel(time_embed_dim=time_embed_dim).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "diffuser = Diffuser(num_timesteps=num_timesteps, device=device)\n",
    "\n",
    "# 学習データ(ガウスノイズ)\n",
    "print(\"############################################\")\n",
    "print(f\"Data_Set_{i+1}, Seed: {seed}\") # 開始の合図\n",
    "\n",
    "print(f\"拡散ステップ数: {num_timesteps}, 学習エポック数: {epochs}, 学習率: {lr}\")\n",
    "print(\"############################################\")\n",
    "np.random.seed(seed) # 取得した乱数を新しいシード値として設定\n",
    "data = np.random.multivariate_normal(original_mean, original_cov, size=dataset_size) # 学習元データの生成 (50, 2)\n",
    "# データの標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data) # shape: (500, 2), dtype: float64\n",
    "\n",
    "print(\"data.shape\", data.shape) # (50, 2)\n",
    "# PyTorchテンソルへ明示的に float32 で変換\n",
    "train_data = torch.tensor(scaled_data, dtype=torch.float32).to(device) # shape: (50, 2)\n",
    "\n",
    "# データローダー作成\n",
    "batch_size = 10\n",
    "# モデル学習に使う DataLoader も float32 のテンソルから作成\n",
    "dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 原点を基準として赤十字\n",
    "plt.axhline(original_mean[0], color='gray', linestyle='--')\n",
    "plt.axvline(original_mean[1], color='gray', linestyle='--')\n",
    "plt.scatter(original_mean[0], original_mean[1], color='red', marker='x', s=100, label='True Mean')\n",
    "# データの可視化\n",
    "plt.scatter(data[:, 0], data[:, 1], label='Original Data')\n",
    "plt.title(f'Original Data (Seed: {seed})')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.legend()\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 標準化したデータの可視化\n",
    "plt.scatter(scaled_data[:, 0], scaled_data[:, 1], label='Scaled Data')\n",
    "plt.title(f'Scaled Data (Seed: {seed})')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# データの要約\n",
    "print(\"dataの平均ベクトル\", np.mean(data, axis=0)) # 平均ベクトル\n",
    "print(\"dataの分散共分散行列\", np.cov(data.T)) # 分散共分散行列\n",
    "print(\"dataの相関係数\", np.corrcoef(data.T)) # 相関係数\n",
    "\n",
    "# 学習\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0.0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch.to(device)\n",
    "        t = torch.randint(1, num_timesteps + 1, (len(x),), device=device)\n",
    "\n",
    "        x_noisy, noise = diffuser.add_noise(x, t)\n",
    "        noise_pred = model(x_noisy, t)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "    avg_loss = loss_sum / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    # 5の倍数エポックで損失を表示\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss}\")\n",
    "# 辞書に保存\n",
    "models[f\"model_{i+1}\"] = model\n",
    "original_datas[f\"seed_{seed}\"] = data\n",
    "print(\"学習終了\")\n",
    "end_time = time.time() # 計測終了\n",
    "print('\\n')\n",
    "print(f\"学習時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "# # モデルの保存\n",
    "# torch.save(model.state_dict(), f\"model_{i+1}.pth\")\n",
    "\n",
    "# 学習曲線のプロット\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Trained By data_by_seed_{}'.format(seed))\n",
    "plt.show()\n",
    "print('\\n')\n",
    "print(\"#\"*50)\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
